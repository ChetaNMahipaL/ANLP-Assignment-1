{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7FBxH3A-tjp"
      },
      "source": [
        "### **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPo5Du-b-tjr",
        "outputId": "8ae85784-3afe-486e-e5b8-3db186ba6416"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2oAAfET-tjs"
      },
      "source": [
        "### **Importing and Cleaning Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVYhbyXZ-tjs",
        "outputId": "ffd98821-e27a-4c48-dd14-0a2fc8be558c"
      },
      "outputs": [],
      "source": [
        "with open('./sample_data/Auguste_Maquet.txt', 'r', encoding='utf-8') as file:\n",
        "    corpus = file.read()\n",
        "\n",
        "print(\"Dataset Loaded\")\n",
        "\n",
        "corpus = corpus.lower()\n",
        "clean_text = sent_tokenize(corpus)\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "clean_text = [sentence.translate(translator) for sentence in clean_text]\n",
        "# print(len(clean_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIpz_vEf-tjs"
      },
      "source": [
        "### **Tokenization and Emmbedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxAnnHsm-tjs",
        "outputId": "23a4a4a3-32f0-4189-9303-72c5ec1245f6"
      },
      "outputs": [],
      "source": [
        "tokenized_corpus = [word_tokenize(sentence) for sentence in clean_text]\n",
        "word_to_ind = {}\n",
        "for i in range(len(tokenized_corpus)):\n",
        "    token_arr = tokenized_corpus[i]\n",
        "\n",
        "    #Vocabulary\n",
        "    for tokken in token_arr:\n",
        "        if tokken not in word_to_ind:\n",
        "            word_to_ind[tokken] = len(word_to_ind)\n",
        "\n",
        "    token_arr = ['<sos>'] * 5 + token_arr + ['<eos>'] * 5\n",
        "    tokenized_corpus[i] = token_arr\n",
        "\n",
        "# print(tokenized_corpus[2])\n",
        "word_to_ind[\"<sos>\"] = len(word_to_ind)\n",
        "word_to_ind[\"<eos>\"] = len(word_to_ind)\n",
        "print(len(word_to_ind))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF0NuxjN-tjs"
      },
      "source": [
        "### **Test-Train Split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jn5Sslyj-tjs",
        "outputId": "204d2b55-be17-4603-de4f-338b32f56dff"
      },
      "outputs": [],
      "source": [
        "train_val_data, test_data = train_test_split(tokenized_corpus, test_size=0.2)\n",
        "\n",
        "train_data, validation_data = train_test_split(train_val_data, test_size=0.125)\n",
        "\n",
        "# Print the sizes of each set\n",
        "print(f\"Training data size: {len(train_data)}\")\n",
        "print(f\"Validation data size: {len(validation_data)}\")\n",
        "print(f\"Test data size: {len(test_data)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtyyUeyS-tjs"
      },
      "source": [
        "### **Neural Network Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXpOSCHX-tjs"
      },
      "outputs": [],
      "source": [
        "class NeuralLM(nn.Module):\n",
        "    def __init__(self, emb_dim, hidden_size, context_size, vocab_size, pretrained_embeddings, dropout_rate):\n",
        "        super(NeuralLM, self).__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.context_size = context_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embeddings = nn.Embedding.from_pretrained(torch.tensor(pretrained_embeddings), freeze=True)\n",
        "        #Model Layers\n",
        "        self.l1 = torch.nn.Linear(self.context_size * self.emb_dim, self.hidden_size)\n",
        "        self.a1 = torch.nn.Tanh()\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.l2 = torch.nn.Linear(self.hidden_size, self.vocab_size)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        #Prepairing Embeddings\n",
        "        inp = self.embeddings(inp)\n",
        "        inp = inp.view(inp.size(1), -1)\n",
        "        # print(inp.shape)\n",
        "\n",
        "        inp = self.l1(inp)\n",
        "        inp = self.a1(inp)\n",
        "        inp = self.dropout(inp)\n",
        "        inp = self.l2(inp)\n",
        "        return inp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ1sj7ul-tjt"
      },
      "source": [
        "### **Creating Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXBWpNWD-tjt"
      },
      "outputs": [],
      "source": [
        "class EntityDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, context_indices, next_word_indices):\n",
        "        self.context_indices = context_indices\n",
        "        self.next_word_indices = next_word_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.next_word_indices)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return torch.tensor(self.context_indices[index]), torch.tensor(self.next_word_indices[index])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnhIaE-Y-tjt"
      },
      "source": [
        "### **Creating Input**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_zbeNGb-tjt",
        "outputId": "591b0488-cdb1-4fca-ed9a-b1eccea5fcfe"
      },
      "outputs": [],
      "source": [
        "N_Gram = 5\n",
        "\n",
        "def process_sentences(sentences, word_to_index, context_size):\n",
        "    def words_to_indices(words, word_to_index):\n",
        "        return [word_to_index.get(word, 0) for word in words]  # Default to 0 if word not in vocab\n",
        "\n",
        "    context_indices = []\n",
        "    central_word_indices = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        word_indices = words_to_indices(sentence, word_to_index)\n",
        "\n",
        "        for i in range(len(sentence) - context_size):\n",
        "            context_window = word_indices[i:i + context_size]\n",
        "            context_indices.append(context_window)\n",
        "            central_word_indices.append(word_to_index.get(sentence[i + context_size], 0))\n",
        "\n",
        "    return context_indices, central_word_indices\n",
        "\n",
        "\n",
        "train_gram_inp, train_cen_inp = process_sentences(train_data, word_to_ind, N_Gram)\n",
        "val_gram_inp, val_cen_inp = process_sentences(validation_data, word_to_ind, N_Gram)\n",
        "test_gram_inp, test_cen_inp = process_sentences(test_data, word_to_ind, N_Gram)\n",
        "\n",
        "print(len(train_cen_inp))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeazJT6A-tjt"
      },
      "source": [
        "### **Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxFKLlhm-tjt"
      },
      "outputs": [],
      "source": [
        "# dataset_train = EntityDataset(train_gram_inp, train_cen_inp)\n",
        "# dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=128)\n",
        "\n",
        "# dataset_val = EntityDataset(val_gram_inp, val_cen_inp)\n",
        "# dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=128)\n",
        "\n",
        "# pretrained_embeddings = word2vec_model.wv.vectors\n",
        "\n",
        "# model = NeuralLM(100, 300, N_Gram, len(word_to_ind), pretrained_embeddings)\n",
        "# model.to(device)\n",
        "\n",
        "# num_epochs = 5\n",
        "# learning_rate = 0.001\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for batch in dataloader_train:\n",
        "#         context_words, target_words = batch\n",
        "#         trans_context = torch.transpose(context_words,0,1)\n",
        "#         trans_context = trans_context.to(device)\n",
        "#         target_words = target_words.to(device)\n",
        "\n",
        "#         outputs = model(trans_context)\n",
        "#         loss = criterion(outputs, target_words)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         optimizer.zero_grad()\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#     avg_train_loss = total_loss / len(dataloader_train)\n",
        "\n",
        "#     # Validation loop\n",
        "#     model.eval()\n",
        "#     total_val_loss = 0\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for batch in dataloader_val:\n",
        "#             context_words, target_words = batch\n",
        "#             trans_context = torch.transpose(context_words,0,1)\n",
        "#             trans_context = trans_context.to(device)\n",
        "#             target_words = target_words.to(device)\n",
        "\n",
        "#             outputs = model(trans_context)\n",
        "#             loss = criterion(outputs, target_words)\n",
        "#             total_val_loss += loss.item()\n",
        "\n",
        "#             _, predicted = torch.max(outputs, 1)\n",
        "#             total += target_words.size(0)\n",
        "#             correct += (predicted == target_words).sum().item()\n",
        "\n",
        "#     avg_val_loss = total_val_loss / len(dataloader_val)\n",
        "#     accuracy = 100 * correct / total\n",
        "\n",
        "#     print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlSUAj71-tjt"
      },
      "source": [
        "### **Evaluate Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0abLW0v-tjt"
      },
      "outputs": [],
      "source": [
        "# dataset_test = EntityDataset(test_gram_inp, test_cen_inp)\n",
        "# dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=128)\n",
        "\n",
        "# model.eval()\n",
        "# correct = 0\n",
        "# total = 0\n",
        "# total_loss = 0\n",
        "# total_tokens = 0\n",
        "# criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in dataloader_test:\n",
        "#         context_words, target_words = batch\n",
        "#         trans_context = torch.transpose(context_words,0,1)\n",
        "#         trans_context = trans_context.to(device)\n",
        "#         target_words = target_words.to(device)\n",
        "\n",
        "#         outputs = model(trans_context)\n",
        "#         # print(outputs)\n",
        "\n",
        "#         loss = criteria(outputs, target_words)\n",
        "#         total_loss += loss.item()\n",
        "#         # total_tokens += target_words.numel()\n",
        "\n",
        "#         _, predicted = torch.max(outputs, 1)\n",
        "#         total += target_words.size(0)\n",
        "#         correct += (predicted == target_words).sum().item()\n",
        "\n",
        "# accuracy = 100 * correct / total\n",
        "# print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "# print(math.exp(total_loss/len(dataloader_test)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "id": "MyqQ3-IT-0qG",
        "outputId": "a66433a6-cc4e-41ca-ac28-65bc8de274d1"
      },
      "outputs": [],
      "source": [
        "# Function to train and validate the model\n",
        "def train_and_evaluate(model, dataloader_train, dataloader_val, criterion, optimizer, num_epochs):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in dataloader_train:\n",
        "            context_words, target_words = batch\n",
        "            # Move data to GPU\n",
        "            trans_context = torch.transpose(context_words, 0, 1).to(device)\n",
        "            target_words = target_words.to(device)\n",
        "\n",
        "            outputs = model(trans_context)\n",
        "            loss = criterion(outputs, target_words)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(dataloader_train)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader_val:\n",
        "                context_words, target_words = batch\n",
        "                # Move data to GPU\n",
        "                trans_context = torch.transpose(context_words, 0, 1).to(device)\n",
        "                target_words = target_words.to(device)\n",
        "\n",
        "                outputs = model(trans_context)\n",
        "                loss = criterion(outputs, target_words)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(dataloader_val)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Function to test the model\n",
        "def test_model(model, dataloader_test, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader_test:\n",
        "            context_words, target_words = batch\n",
        "            # Move data to GPU\n",
        "            trans_context = torch.transpose(context_words, 0, 1).to(device)\n",
        "            target_words = target_words.to(device)\n",
        "\n",
        "            outputs = model(trans_context)\n",
        "            loss = criterion(outputs, target_words)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += target_words.size(0)\n",
        "            correct += (predicted == target_words).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    perplexity = math.exp(total_loss / len(dataloader_test))\n",
        "\n",
        "    return accuracy, perplexity\n",
        "\n",
        "# Hyperparameter variations\n",
        "combinations = [\n",
        "    (0.1, 100, 'Adam', 0.001),\n",
        "    (0.3, 200, 'Adam', 0.001),\n",
        "    (0.1, 300, 'SGD', 0.001),\n",
        "    (0.3, 100, 'SGD', 0.01),\n",
        "    (0.5, 200, 'Adam', 0.001)\n",
        "]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_epochs = 5\n",
        "\n",
        "# Store results for plotting\n",
        "train_perplexities = []\n",
        "val_perplexities = []\n",
        "test_perplexities = []\n",
        "\n",
        "dataset_train = EntityDataset(train_gram_inp, train_cen_inp)\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=128)\n",
        "\n",
        "dataset_val = EntityDataset(val_gram_inp, val_cen_inp)\n",
        "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=128)\n",
        "\n",
        "dataset_test = EntityDataset(test_gram_inp, test_cen_inp)\n",
        "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=128)\n",
        "\n",
        "# Open file to save the final perplexities and hyperparameters for report\n",
        "with open(\"perplexity_report.txt\", \"w\") as f:\n",
        "    for (dropout, dim, opt, lr) in combinations:\n",
        "        print(f\"Training with Dropout={dropout}, Dimension={dim}, Optimizer={opt}, Learning Rate={lr}\")\n",
        "        f.write(f\"Training with Dropout={dropout}, Dimension={dim}, Optimizer={opt}, Learning Rate={lr}\\n\")\n",
        "\n",
        "        word2vec_model = Word2Vec(sentences=tokenized_corpus, vector_size=dim, window=5, min_count=1, workers=4)\n",
        "        pretrained_embeddings = word2vec_model.wv.vectors\n",
        "\n",
        "        # Initialize model with different hyperparameters\n",
        "        model = NeuralLM(dim, 300, N_Gram, len(word_to_ind), pretrained_embeddings, dropout_rate=dropout)\n",
        "        model.to(device)\n",
        "\n",
        "        # Set optimizer\n",
        "        if opt == 'Adam':\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        elif opt == 'SGD':\n",
        "            optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "        # Train and evaluate the model\n",
        "        train_losses, val_losses = train_and_evaluate(model, dataloader_train, dataloader_val, criterion, optimizer, num_epochs)\n",
        "\n",
        "        # Compute perplexity for each epoch\n",
        "        train_perplexities.append([math.exp(loss) for loss in train_losses])\n",
        "        val_perplexities.append([math.exp(loss) for loss in val_losses])\n",
        "\n",
        "        # Test the model\n",
        "        test_acc, test_perplexity = test_model(model, dataloader_test, criterion)\n",
        "        test_perplexities.append(test_perplexity)\n",
        "\n",
        "        # Write final test perplexity for this combination to the report file\n",
        "        f.write(f\"Final Test Perplexity: {test_perplexity}\\n\")\n",
        "        f.write(\"=\"*50 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QgwCNbBC_5Kj",
        "outputId": "1adb6474-6c72-4327-8f70-28dcbb0a9633"
      },
      "outputs": [],
      "source": [
        "# Plotting the perplexities\n",
        "epochs = list(range(1, num_epochs + 1))\n",
        "for i, (dropout, dim, opt, lr) in enumerate(combinations):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, train_perplexities[i], label='Train Perplexity')\n",
        "    plt.plot(epochs, val_perplexities[i], label='Val Perplexity')\n",
        "    plt.title(f'Dropout={dropout}, Dimension={dim}, Optimizer={opt}, Learning Rate={lr}')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Perplexity')\n",
        "    plt.legend()\n",
        "    # plt.show()\n",
        "    plt.savefig(f'./plot_{i}.png')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
