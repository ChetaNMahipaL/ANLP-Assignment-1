{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing and Cleaning Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/Auguste_Maquet.txt', 'r', encoding='utf-8') as file:\n",
    "    corpus = file.read()\n",
    "\n",
    "corpus = corpus.lower()\n",
    "clean_text = sent_tokenize(corpus)\n",
    "print(len(clean_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenization and Emmbedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [word_tokenize(sentence) for sentence in clean_text]\n",
    "word_to_ind = {}\n",
    "longest_seq = 1\n",
    "for i in range(len(tokenized_corpus)):\n",
    "    token_arr = tokenized_corpus[i]\n",
    "    longest_seq = max(longest_seq, len(token_arr))\n",
    "    \n",
    "    #Vocabulary\n",
    "    for tokken in token_arr:\n",
    "        if tokken not in word_to_ind:\n",
    "            word_to_ind[tokken] = len(word_to_ind)\n",
    "    \n",
    "    token_arr = ['<sos>'] * 5 + token_arr + ['<eos>'] * 5\n",
    "    tokenized_corpus[i] = token_arr\n",
    "\n",
    "# print(tokenized_corpus[2])\n",
    "word_to_ind[\"<sos>\"] = len(word_to_ind)\n",
    "word_to_ind[\"<eos>\"] = len(word_to_ind)\n",
    "print(len(word_to_ind))\n",
    "\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=tokenized_corpus, vector_size=200, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Test-Train Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_data, test_data = train_test_split(tokenized_corpus, test_size=0.2)\n",
    "\n",
    "train_data, validation_data = train_test_split(train_val_data, test_size=0.125)\n",
    "\n",
    "# Print the sizes of each set\n",
    "print(f\"Training data size: {len(train_data)}\")\n",
    "print(f\"Validation data size: {len(validation_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Encoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Positional Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEncoding(nn.Module):\n",
    "    def __init__(self, model_dim, max_len):\n",
    "        super(PosEncoding, self).__init__()\n",
    "        pos_code = torch.zeros(max_len, model_dim).to(device)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1).to(device)\n",
    "        scale = torch.exp(torch.arange(0, model_dim, 2, dtype=torch.float) * \n",
    "                          -(math.log(10000.0) / model_dim)).to(device)\n",
    "        \n",
    "        pos_code[:, 0::2] = torch.sin(pos * scale)\n",
    "        pos_code[:, 1::2] = torch.cos(pos * scale)\n",
    "        \n",
    "        # Registering the positional encoding matrix as a buffer to avoid updating during training\n",
    "        self.register_buffer('pos_code', pos_code.unsqueeze(0))\n",
    "\n",
    "    def forward(self, inp):\n",
    "        seq_len = inp.size(1)\n",
    "        inp = inp.to(device) + self.pos_code[:, :seq_len]\n",
    "        \n",
    "        return inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multi Head Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, model_dim, num_layers):\n",
    "#         super(MultiHeadAttention, self).__init__()\n",
    "#         self.model_dim = model_dim\n",
    "#         self.num_layers = num_layers\n",
    "#         self.dim_key = self.model_dim // self.num_layers\n",
    "\n",
    "#         # Linear layers for query, key, and value\n",
    "#         self.query = nn.Linear(model_dim, model_dim).to(device)\n",
    "#         self.key = nn.Linear(model_dim, model_dim).to(device)\n",
    "#         self.value = nn.Linear(model_dim, model_dim).to(device)\n",
    "#         self.out = nn.Linear(model_dim, model_dim).to(device)\n",
    "\n",
    "#     def attention_val(self, Q, K, V, mask=None):\n",
    "#         score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.dim_key)\n",
    "\n",
    "#         if mask is not None:\n",
    "#             score = score.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "#         attn_weight = torch.softmax(score, dim=-1)\n",
    "\n",
    "#         new_val = torch.matmul(attn_weight, V)\n",
    "#         return new_val\n",
    "\n",
    "#     def split_layers(self, x):\n",
    "#         batch_size, seq_len, model_dim = x.size()  \n",
    "#         return x.view(batch_size, seq_len, self.num_layers, self.dim_key).transpose(1, 2)\n",
    "\n",
    "#     def combine_layers(self, x):\n",
    "#         batch_size, _, seq_len, dim_key = x.size()\n",
    "#         return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.model_dim)\n",
    "\n",
    "#     def forward(self, Q, K, V, mask=None):\n",
    "#         # Split into multiple heads\n",
    "#         Q = self.split_layers(self.query(Q).to(device))\n",
    "#         K = self.split_layers(self.key(K).to(device))\n",
    "#         V = self.split_layers(self.value(V).to(device))\n",
    "\n",
    "#         layer_out = self.attention_val(Q, K, V, mask)\n",
    "#         final_output = self.out(self.combine_layers(layer_out)).to(device)\n",
    "        \n",
    "#         return final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **FeedForward**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self, model_dim, hid_dim):\n",
    "#         super(FeedForward, self).__init__()\n",
    "#         self.l1 = nn.Linear(model_dim, hid_dim).to(device)\n",
    "#         self.ac1 = nn.ReLU().to(device)\n",
    "#         self.l2 = nn.Linear(hid_dim, model_dim).to(device)\n",
    "\n",
    "#     def forward(self, inp):\n",
    "#         inp = self.l1(inp).to(device)\n",
    "#         inp = self.ac1(inp).to(device)\n",
    "#         inp = self.l2(inp).to(device)\n",
    "#         return inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Combining Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, model_dim, num_layers, hid_dim, dropout):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.self_attn = MultiHeadAttention(model_dim, num_layers).to(device)\n",
    "#         self.norm1 = nn.LayerNorm(model_dim).to(device)\n",
    "#         self.ffn = FeedForward(model_dim, hid_dim).to(device)\n",
    "#         self.norm2 = nn.LayerNorm(model_dim).to(device)\n",
    "#         self.dropout = nn.Dropout(dropout).to(device)\n",
    "\n",
    "#     def forward(self, inp, mask):\n",
    "\n",
    "#         att_score = self.self_attn(inp, inp, inp, mask).to(device)\n",
    "#         inp = self.norm1(inp + self.dropout(att_score).to(device)).to(device)\n",
    "#         ffn_out = self.ffn(inp).to(device)\n",
    "#         inp = self.norm2(inp + self.dropout(ffn_out).to(device)).to(device)\n",
    "        \n",
    "#         return inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Decoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Combining Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, model_dim, num_layers, hid_dim, dropout):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.self_attn = MultiHeadAttention(model_dim, num_layers).to(device)\n",
    "#         self.norm1 = nn.LayerNorm(model_dim).to(device)\n",
    "#         self.cross_attn = MultiHeadAttention(model_dim, num_layers).to(device)\n",
    "#         self.norm2 = nn.LayerNorm(model_dim).to(device)\n",
    "#         self.ffn = FeedForward(model_dim, hid_dim).to(device)\n",
    "#         self.norm3 = nn.LayerNorm(model_dim).to(device)\n",
    "#         self.dropout = nn.Dropout(dropout).to(device)\n",
    "\n",
    "#     def forward(self, inp, encoder_out, source_mask, target_mask):\n",
    "\n",
    "#         att_score = self.self_attn(inp, inp, inp, target_mask).to(device)\n",
    "#         inp = self.norm1(inp + self.dropout(att_score).to(device)).to(device)\n",
    "#         att_score_cross = self.cross_attn(inp, encoder_out, encoder_out, source_mask).to(device)\n",
    "#         inp = self.norm2(inp + self.dropout(att_score_cross).to(device)).to(device)\n",
    "#         ffn_out = self.ffn(inp).to(device)\n",
    "#         inp = self.norm3(inp + self.dropout(ffn_out).to(device)).to(device)\n",
    "\n",
    "#         return inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dim, num_layers, num_times, hid_dim, max_len, dropout, pretrained_embeddings):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoderEmb = nn.Embedding.from_pretrained(torch.tensor(pretrained_embeddings), freeze=True).to(device)\n",
    "        self.pos_enc = PosEncoding(model_dim, max_len).to(device)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_layers, dim_feedforward=hid_dim, dropout=dropout)\n",
    "        self.enclayer = nn.TransformerEncoder(encoder_layer, num_layers=num_times).to(device)\n",
    "\n",
    "        self.decoderEmb = nn.Embedding.from_pretrained(torch.tensor(pretrained_embeddings), freeze=True).to(device)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=model_dim, nhead=num_layers, dim_feedforward=hid_dim, dropout=dropout)\n",
    "        self.declayer = nn.TransformerDecoder(decoder_layer, num_layers=num_times).to(device)\n",
    "\n",
    "        self.final_layer = nn.Linear(model_dim, vocab_size).to(device)\n",
    "        self.dropout = nn.Dropout(dropout).to(device)\n",
    "\n",
    "    def masking(self, src, target):\n",
    "        source_mask = (src != 0).unsqueeze(1).unsqueeze(2).to(device)\n",
    "        target_mask = (target != 0).unsqueeze(1).unsqueeze(3).to(device)\n",
    "        length = target.size(1)\n",
    "        \n",
    "        no_mask = (1 - torch.triu(torch.ones(1, length, length), diagonal=1)).bool().to(device)\n",
    "        target_mask = target_mask & no_mask\n",
    "        return source_mask, target_mask\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        source_mask, target_mask = self.masking(source, target)\n",
    "\n",
    "        source_emb = self.pos_enc(self.encoderEmb(source).to(device))\n",
    "        source_emb = self.dropout(source_emb).to(device)\n",
    "\n",
    "        target_emb = self.pos_enc(self.decoderEmb(target).to(device))\n",
    "        target_emb = self.dropout(target_emb).to(device)\n",
    "\n",
    "        enc_out = self.enclayer(source_emb, src_key_padding_mask=source_mask.squeeze(1).squeeze(1)).to(device)\n",
    "\n",
    "        dec_out = self.declayer(target_emb, enc_out, tgt_mask=target_mask, memory_mask=source_mask).to(device)\n",
    "\n",
    "        final_out = self.final_layer(dec_out).to(device)\n",
    "        \n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sentences, targets, max_len=None):\n",
    "        self.sentences = sentences\n",
    "        self.targets = targets\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        # Padding or truncation to max_len if specified\n",
    "        if self.max_len is not None:\n",
    "            sentence = sentence[:self.max_len] + [0] * max(0, self.max_len - len(sentence))\n",
    "            target = target[:self.max_len] + [0] * max(0, self.max_len - len(target))\n",
    "\n",
    "        sentence_tensor = torch.tensor(sentence, dtype=torch.long)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "        return sentence_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating Input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(sentences, word_to_index, max_len=None):\n",
    "    def words_to_indices(words, word_to_index):\n",
    "        return [word_to_index.get(word, 0) for word in words]\n",
    "    \n",
    "    context_indices = []\n",
    "    central_word_indices = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        word_indices = words_to_indices(sentence, word_to_index)\n",
    "        \n",
    "        if max_len is not None:\n",
    "            word_indices = word_indices[:max_len] + [0] * (max_len - len(word_indices))\n",
    "\n",
    "        context_indices.append(word_indices[:-1])\n",
    "        \n",
    "        central_word_indices.append(word_indices[1:])\n",
    "\n",
    "    return context_indices, central_word_indices\n",
    "\n",
    "train_gram_inp, train_cen_inp = process_sentences(train_data, word_to_ind, max_len=20)\n",
    "val_gram_inp, val_cen_inp = process_sentences(validation_data, word_to_ind, max_len=20)\n",
    "test_gram_inp, test_cen_inp = process_sentences(test_data, word_to_ind, max_len=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = LM_Dataset(train_gram_inp, train_cen_inp, max_len=longest_seq)  \n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "\n",
    "dataset_val = LM_Dataset(val_gram_inp, val_cen_inp, max_len=longest_seq)\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=128)\n",
    "\n",
    "pretrained_embeddings = word2vec_model.wv.vectors\n",
    "\n",
    "model = Transformer(vocab_size=len(word_to_ind), model_dim=200, num_layers=4, num_times=6, hid_dim=300, max_len=longest_seq, dropout=0.1, pretrained_embeddings=pretrained_embeddings)\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader_train:\n",
    "        context_words, target_words = batch\n",
    "        context_words = context_words.to(device)\n",
    "        target_words = target_words.to(device)\n",
    "\n",
    "        outputs = model(context_words, target_words) \n",
    "        \n",
    "        outputs = outputs.view(-1, outputs.size(-1))  \n",
    "        target_words = target_words.view(-1)\n",
    "        loss = criterion(outputs, target_words)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(dataloader_train)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader_val:\n",
    "            context_words, target_words = batch\n",
    "            context_words = context_words.to(device)\n",
    "            target_words = target_words.to(device)\n",
    "            \n",
    "            outputs = model(context_words, target_words)\n",
    "            \n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "            target_words = target_words.view(-1)\n",
    "            loss = criterion(outputs, target_words)\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += target_words.size(0)\n",
    "            correct += (predicted == target_words).sum().item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(dataloader_val)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluate Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_test = LM_Dataset(test_gram_inp, test_cen_inp)\n",
    "# dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=128)\n",
    "\n",
    "# model.eval()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# total_loss = 0\n",
    "# total_tokens = 0\n",
    "# criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in dataloader_test:\n",
    "#         context_words, target_words = batch\n",
    "#         context_words = context_words.to(device)\n",
    "#         target_words = target_words.to(device)\n",
    "\n",
    "#         outputs = model(context_words)  \n",
    "#         outputs = outputs.view(-1, outputs.size(-1))\n",
    "#         target_words = target_words.view(-1)\n",
    "        \n",
    "#         loss = criterion(outputs, target_words)\n",
    "#         total_loss += loss.item()\n",
    "#         # total_tokens += target_words.numel()\n",
    "        \n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         total += target_words.size(0)\n",
    "#         correct += (predicted == target_words).sum().item()\n",
    "\n",
    "# accuracy = 100 * correct / total\n",
    "# print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "# print(math.exp(total_loss/len(dataloader_test)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
