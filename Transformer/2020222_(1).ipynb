{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5UFk3gjVFdg"
      },
      "source": [
        "### **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fL5sQ-dVFdi",
        "outputId": "62163232-fdaa-4acb-d85e-ec51a3d8fac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import math\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import string\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-M5epDOVFdj"
      },
      "source": [
        "### **Importing and Cleaning Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsonTicJVFdk",
        "outputId": "a6f166c5-c220-475c-e3ef-ec76acb143ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded\n"
          ]
        }
      ],
      "source": [
        "with open('./sample_data/Auguste_Maquet.txt', 'r', encoding='utf-8') as file:\n",
        "    corpus = file.read()\n",
        "\n",
        "print(\"Dataset Loaded\")\n",
        "\n",
        "corpus = corpus.lower()\n",
        "clean_text = sent_tokenize(corpus)\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "clean_text = [sentence.translate(translator) for sentence in clean_text]\n",
        "# print(len(clean_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cdhQwLDVFdk"
      },
      "source": [
        "### **Tokenization and Emmbedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-kkc85fVFdk",
        "outputId": "509ab2c2-1e2f-44e8-8232-1e5cbf5677a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26161\n"
          ]
        }
      ],
      "source": [
        "tokenized_corpus = [word_tokenize(sentence) for sentence in clean_text]\n",
        "word_to_ind = {}\n",
        "longest_seq = 1\n",
        "for i in range(len(tokenized_corpus)):\n",
        "    token_arr = tokenized_corpus[i]\n",
        "    longest_seq = max(longest_seq, len(token_arr))\n",
        "\n",
        "    #Vocabulary\n",
        "    for tokken in token_arr:\n",
        "        if tokken not in word_to_ind:\n",
        "            word_to_ind[tokken] = len(word_to_ind)\n",
        "\n",
        "    token_arr = ['<sos>'] * 5 + token_arr + ['<eos>'] * 5\n",
        "    tokenized_corpus[i] = token_arr\n",
        "\n",
        "# print(tokenized_corpus[2])\n",
        "word_to_ind[\"<sos>\"] = len(word_to_ind)\n",
        "word_to_ind[\"<eos>\"] = len(word_to_ind)\n",
        "print(len(word_to_ind))\n",
        "\n",
        "\n",
        "word2vec_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpjaWOGJVFdk"
      },
      "source": [
        "### **Test-Train Split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxGRwOfHVFdk",
        "outputId": "1ec01a11-3ce7-467b-bedd-9a1d5cbebbe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data size: 24571\n",
            "Validation data size: 3511\n",
            "Test data size: 7021\n"
          ]
        }
      ],
      "source": [
        "train_val_data, test_data = train_test_split(tokenized_corpus, test_size=0.2)\n",
        "\n",
        "train_data, validation_data = train_test_split(train_val_data, test_size=0.125)\n",
        "\n",
        "# Print the sizes of each set\n",
        "print(f\"Training data size: {len(train_data)}\")\n",
        "print(f\"Validation data size: {len(validation_data)}\")\n",
        "print(f\"Test data size: {len(test_data)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5AGJfKQVFdl"
      },
      "source": [
        "## **Encoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaOuRTZUVFdl"
      },
      "source": [
        "### **Positional Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "w3oAGT49VFdl"
      },
      "outputs": [],
      "source": [
        "class PosEncoding(nn.Module):\n",
        "    def __init__(self, model_dim, max_len):\n",
        "        super(PosEncoding, self).__init__()\n",
        "        pos_code = torch.zeros(max_len, model_dim).to(device)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1).to(device)\n",
        "        scale = torch.exp(torch.arange(0, model_dim, 2, dtype=torch.float) *\n",
        "                          -(math.log(10000.0) / model_dim)).to(device)\n",
        "\n",
        "        pos_code[:, 0::2] = torch.sin(pos * scale)\n",
        "        pos_code[:, 1::2] = torch.cos(pos * scale)\n",
        "\n",
        "        # Registering the positional encoding matrix as a buffer to avoid updating during training\n",
        "        self.register_buffer('pos_code', pos_code.unsqueeze(0))\n",
        "\n",
        "    def forward(self, inp):\n",
        "        seq_len = inp.size(1)\n",
        "        inp = inp.to(device) + self.pos_code[:, :seq_len]\n",
        "\n",
        "        return inp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFmGvWLHVFdl"
      },
      "source": [
        "### **Multi Head Attention**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YFbih_nXVFdl"
      },
      "outputs": [],
      "source": [
        "# class MultiHeadAttention(nn.Module):\n",
        "#     def __init__(self, model_dim, num_layers):\n",
        "#         super(MultiHeadAttention, self).__init__()\n",
        "#         self.model_dim = model_dim\n",
        "#         self.num_layers = num_layers\n",
        "#         self.dim_key = self.model_dim // self.num_layers\n",
        "\n",
        "#         # Linear layers for query, key, and value\n",
        "#         self.query = nn.Linear(model_dim, model_dim).to(device)\n",
        "#         self.key = nn.Linear(model_dim, model_dim).to(device)\n",
        "#         self.value = nn.Linear(model_dim, model_dim).to(device)\n",
        "#         self.out = nn.Linear(model_dim, model_dim).to(device)\n",
        "\n",
        "#     def attention_val(self, Q, K, V, mask=None):\n",
        "#         score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.dim_key)\n",
        "\n",
        "#         if mask is not None:\n",
        "#             score = score.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "#         attn_weight = torch.softmax(score, dim=-1)\n",
        "\n",
        "#         new_val = torch.matmul(attn_weight, V)\n",
        "#         return new_val\n",
        "\n",
        "#     def split_layers(self, x):\n",
        "#         batch_size, seq_len, model_dim = x.size()\n",
        "#         return x.view(batch_size, seq_len, self.num_layers, self.dim_key).transpose(1, 2)\n",
        "\n",
        "#     def combine_layers(self, x):\n",
        "#         batch_size, _, seq_len, dim_key = x.size()\n",
        "#         return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.model_dim)\n",
        "\n",
        "#     def forward(self, Q, K, V, mask=None):\n",
        "#         # Split into multiple heads\n",
        "#         Q = self.split_layers(self.query(Q).to(device))\n",
        "#         K = self.split_layers(self.key(K).to(device))\n",
        "#         V = self.split_layers(self.value(V).to(device))\n",
        "\n",
        "#         layer_out = self.attention_val(Q, K, V, mask)\n",
        "#         final_output = self.out(self.combine_layers(layer_out)).to(device)\n",
        "\n",
        "#         return final_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuueGbFIVFdm"
      },
      "source": [
        "### **FeedForward**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oYifHE8fVFdm"
      },
      "outputs": [],
      "source": [
        "# class FeedForward(nn.Module):\n",
        "#     def __init__(self, model_dim, hid_dim):\n",
        "#         super(FeedForward, self).__init__()\n",
        "#         self.l1 = nn.Linear(model_dim, hid_dim).to(device)\n",
        "#         self.ac1 = nn.ReLU().to(device)\n",
        "#         self.l2 = nn.Linear(hid_dim, model_dim).to(device)\n",
        "\n",
        "#     def forward(self, inp):\n",
        "#         inp = self.l1(inp).to(device)\n",
        "#         inp = self.ac1(inp).to(device)\n",
        "#         inp = self.l2(inp).to(device)\n",
        "#         return inp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgbmTBODVFdm"
      },
      "source": [
        "### **Combining Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mbdPJZmFVFdm"
      },
      "outputs": [],
      "source": [
        "# class Encoder(nn.Module):\n",
        "#     def __init__(self, model_dim, num_layers, hid_dim, dropout):\n",
        "#         super(Encoder, self).__init__()\n",
        "#         self.self_attn = MultiHeadAttention(model_dim, num_layers).to(device)\n",
        "#         self.norm1 = nn.LayerNorm(model_dim).to(device)\n",
        "#         self.ffn = FeedForward(model_dim, hid_dim).to(device)\n",
        "#         self.norm2 = nn.LayerNorm(model_dim).to(device)\n",
        "#         self.dropout = nn.Dropout(dropout).to(device)\n",
        "\n",
        "#     def forward(self, inp, mask):\n",
        "\n",
        "#         att_score = self.self_attn(inp, inp, inp, mask).to(device)\n",
        "#         inp = self.norm1(inp + self.dropout(att_score).to(device)).to(device)\n",
        "#         ffn_out = self.ffn(inp).to(device)\n",
        "#         inp = self.norm2(inp + self.dropout(ffn_out).to(device)).to(device)\n",
        "\n",
        "#         return inp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx4WjCqBVFdm"
      },
      "source": [
        "## **Decoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmTj8mXYVFdm"
      },
      "source": [
        "### **Combining Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "frJba1nzVFdn"
      },
      "outputs": [],
      "source": [
        "# class Decoder(nn.Module):\n",
        "#     def __init__(self, model_dim, num_layers, hid_dim, dropout):\n",
        "#         super(Decoder, self).__init__()\n",
        "#         self.self_attn = MultiHeadAttention(model_dim, num_layers).to(device)\n",
        "#         self.norm1 = nn.LayerNorm(model_dim).to(device)\n",
        "#         self.cross_attn = MultiHeadAttention(model_dim, num_layers).to(device)\n",
        "#         self.norm2 = nn.LayerNorm(model_dim).to(device)\n",
        "#         self.ffn = FeedForward(model_dim, hid_dim).to(device)\n",
        "#         self.norm3 = nn.LayerNorm(model_dim).to(device)\n",
        "#         self.dropout = nn.Dropout(dropout).to(device)\n",
        "\n",
        "#     def forward(self, inp, encoder_out, source_mask, target_mask):\n",
        "\n",
        "#         att_score = self.self_attn(inp, inp, inp, target_mask).to(device)\n",
        "#         inp = self.norm1(inp + self.dropout(att_score).to(device)).to(device)\n",
        "#         att_score_cross = self.cross_attn(inp, encoder_out, encoder_out, source_mask).to(device)\n",
        "#         inp = self.norm2(inp + self.dropout(att_score_cross).to(device)).to(device)\n",
        "#         ffn_out = self.ffn(inp).to(device)\n",
        "#         inp = self.norm3(inp + self.dropout(ffn_out).to(device)).to(device)\n",
        "\n",
        "#         return inp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61qUfIVGVFdn"
      },
      "source": [
        "## **Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Rq1O4ZHlVFdn"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, model_dim, num_layer, num_heads, hid_dim, max_len, dropout=0.1, pretrained_embeddings=None):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.model_dim = model_dim\n",
        "        if pretrained_embeddings is not None:\n",
        "            self.embed = nn.Embedding.from_pretrained(torch.tensor(pretrained_embeddings, dtype=torch.float), freeze=True)\n",
        "        else:\n",
        "            self.embed = nn.Embedding(vocab_size, model_dim)\n",
        "\n",
        "        self.pos_enc = PosEncoding(model_dim, max_len)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=model_dim, nhead=num_heads, dim_feedforward=hid_dim, dropout=dropout, batch_first=True)\n",
        "        self.declayer = nn.TransformerDecoder(decoder_layer, num_layers=num_layer)\n",
        "\n",
        "        self.final_layer = nn.Linear(model_dim, vocab_size)\n",
        "        self.max_len = max_len\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, src, target):\n",
        "        target = target.to(device)\n",
        "\n",
        "        target_mask = self.generate_square_subsequent_mask(target.size(1)).to(device)\n",
        "\n",
        "        # target_emb = self.pos_enc(self.encoderEmb(target))\n",
        "        # padding_mask = (target == 0).to(device)\n",
        "        target_emb = self.pos_enc(self.embed(target) / math.sqrt(self.model_dim)).to(device)\n",
        "\n",
        "        dec_out = self.declayer(target_emb, src, tgt_mask=target_mask)\n",
        "\n",
        "        final_out = self.final_layer(dec_out)\n",
        "        return final_out\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDJoaqvdVFdn"
      },
      "source": [
        "### **Creating Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "X1KPf2g_VFdn"
      },
      "outputs": [],
      "source": [
        "class LM_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sentences, targets):\n",
        "        self.sentences = sentences\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        target = self.targets[idx]\n",
        "\n",
        "        sentence_tensor = torch.tensor(sentence, dtype=torch.long)\n",
        "        target_tensor = torch.tensor(target, dtype=torch.long)\n",
        "\n",
        "        return sentence_tensor, target_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYp-ZUfmVFdn"
      },
      "source": [
        "### **Creating Input**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LGlKjcNzVFdn"
      },
      "outputs": [],
      "source": [
        "# def process_sentences(sentences, word_to_index, max_len=None):\n",
        "#     def words_to_indices(words, word_to_index):\n",
        "#         return [word_to_index.get(word, 0) for word in words]\n",
        "\n",
        "#     context_indices = []\n",
        "#     central_word_indices = []\n",
        "\n",
        "#     for sentence in sentences:\n",
        "#         word_indices = words_to_indices(sentence, word_to_index)\n",
        "\n",
        "#         if max_len is not None:\n",
        "#             word_indices = word_indices[:max_len] + [0] * (max_len - len(word_indices))\n",
        "\n",
        "#         context_indices.append(word_indices[:-1])\n",
        "\n",
        "#         central_word_indices.append(word_indices[1:])\n",
        "\n",
        "#     return context_indices, central_word_indices\n",
        "\n",
        "# train_gram_inp, train_cen_inp = process_sentences(train_data, word_to_ind, max_len=20)\n",
        "# val_gram_inp, val_cen_inp = process_sentences(validation_data, word_to_ind, max_len=20)\n",
        "# test_gram_inp, test_cen_inp = process_sentences(test_data, word_to_ind, max_len=20)\n",
        "# print(train_cen_inp[0])\n",
        "\n",
        "def process_sentences(sentences, word_to_index, max_len=None):\n",
        "    def words_to_indices(words, word_to_index):\n",
        "        return [word_to_index.get(word, 0) for word in words]\n",
        "\n",
        "    context_indices = []\n",
        "    central_word_indices = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        word_indices = words_to_indices(sentence, word_to_index)\n",
        "\n",
        "        if max_len is not None:\n",
        "            if len(word_indices) < max_len:\n",
        "                word_indices = word_indices + [0] * (max_len - len(word_indices))\n",
        "            else:\n",
        "                word_indices = word_indices[:max_len]\n",
        "\n",
        "        context_indices.append(word_indices[:-1])\n",
        "        central_word_indices.append(word_indices[1:])\n",
        "\n",
        "    return context_indices, central_word_indices\n",
        "\n",
        "train_gram_inp, train_cen_inp = process_sentences(train_data, word_to_ind, max_len=20)\n",
        "val_gram_inp, val_cen_inp = process_sentences(validation_data, word_to_ind, max_len=20)\n",
        "test_gram_inp, test_cen_inp = process_sentences(test_data, word_to_ind, max_len=20)\n",
        "\n",
        "# Example output\n",
        "# print(train_cen_inp[0])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGNFQRZ5VFdo"
      },
      "source": [
        "### **Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xF5uSy-VFdo",
        "outputId": "654880e2-c513-4555-9f15-f56dd47d45fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Train Loss: 4.3679, Val Loss: 3.9752, Val Accuracy: 39.28%\n",
            "Epoch [2/10], Train Loss: 3.8375, Val Loss: 3.8108, Val Accuracy: 40.59%\n",
            "Epoch [3/10], Train Loss: 3.6723, Val Loss: 3.7316, Val Accuracy: 41.24%\n",
            "Epoch [4/10], Train Loss: 3.5624, Val Loss: 3.6966, Val Accuracy: 41.75%\n",
            "Epoch [5/10], Train Loss: 3.4689, Val Loss: 3.6955, Val Accuracy: 41.85%\n",
            "Epoch [6/10], Train Loss: 3.4357, Val Loss: 3.8171, Val Accuracy: 40.82%\n",
            "Epoch [7/10], Train Loss: 3.3803, Val Loss: 3.6750, Val Accuracy: 42.32%\n",
            "Epoch [8/10], Train Loss: 3.2768, Val Loss: 3.7047, Val Accuracy: 42.39%\n",
            "Epoch [9/10], Train Loss: 3.2283, Val Loss: 3.6899, Val Accuracy: 42.78%\n",
            "Epoch [10/10], Train Loss: 3.1824, Val Loss: 3.7015, Val Accuracy: 43.07%\n"
          ]
        }
      ],
      "source": [
        "dataset_train = LM_Dataset(train_gram_inp, train_cen_inp)\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
        "\n",
        "dataset_val = LM_Dataset(val_gram_inp, val_cen_inp)\n",
        "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=16)\n",
        "\n",
        "pretrained_embeddings = word2vec_model.wv.vectors\n",
        "\n",
        "model = Transformer(vocab_size=len(word_to_ind), model_dim=100, num_heads=4, num_layer=6, hid_dim=300, max_len=40, dropout=0.1, pretrained_embeddings=pretrained_embeddings)\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader_train:\n",
        "        context_words, target_words = batch\n",
        "        context_words = context_words.to(device)\n",
        "        target_words = target_words.to(device)\n",
        "        src = torch.zeros(context_words.size(0), 1, model.model_dim).to(context_words.device)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(src, context_words)\n",
        "        # Check the shapes and a few values of the outputs\n",
        "        # print(f\"Outputs shape: {outputs.shape}\")\n",
        "        # print(f\"Outputs sample: {outputs[0, :5]}\")  # Print first few values of the first batch\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.size(-1))\n",
        "        target_words = target_words.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, target_words)\n",
        "        # print(f\"Loss: {loss.item()}\")\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(dataloader_train)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader_val:\n",
        "            context_words, target_words = batch\n",
        "            context_words = context_words.to(device)\n",
        "            target_words = target_words.to(device)\n",
        "            src = torch.zeros(context_words.size(0), 1, model.model_dim).to(context_words.device)\n",
        "\n",
        "\n",
        "\n",
        "            outputs = model(src, context_words)\n",
        "            outputs = outputs.view(-1, outputs.size(-1))\n",
        "            target_words = target_words.view(-1)\n",
        "            loss = criterion(outputs, target_words)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += target_words.size(0)\n",
        "            correct += (predicted == target_words).sum().item()\n",
        "\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(dataloader_val)\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {accuracy:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI_4BWWCVFdo"
      },
      "source": [
        "### **Evaluate Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6obGWhHdVFdo",
        "outputId": "39244398-f63b-47ae-e408-418bd9f13e19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 43.31%\n",
            "Average Test Loss: 3.6676\n",
            "Perplexity: 39.16\n"
          ]
        }
      ],
      "source": [
        "dataset_test = LM_Dataset(test_gram_inp, test_cen_inp)\n",
        "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=16)\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "total_loss = 0\n",
        "total_tokens = 0\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in dataloader_test:\n",
        "        context_words, target_words = batch\n",
        "        context_words = context_words.to(device)\n",
        "        target_words = target_words.to(device)\n",
        "        src = torch.zeros(context_words.size(0), 1, model.model_dim).to(context_words.device)\n",
        "\n",
        "\n",
        "        outputs = model(src, context_words)\n",
        "        outputs = outputs.view(-1, outputs.size(-1))\n",
        "        target_words = target_words.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, target_words)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += target_words.size(0)\n",
        "        correct += (predicted == target_words).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "average_loss = total_loss / len(dataloader_test)\n",
        "perplexity = math.exp(average_loss)\n",
        "\n",
        "print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "print(f'Average Test Loss: {average_loss:.4f}')\n",
        "print(f'Perplexity: {perplexity:.2f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}