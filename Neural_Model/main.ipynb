{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing and Cleaning Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/Auguste_Maquet.txt', 'r', encoding='utf-8') as file:\n",
    "    corpus = file.read()\n",
    "\n",
    "corpus = corpus.lower()\n",
    "clean_text = sent_tokenize(corpus)\n",
    "print(len(clean_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenization and Emmbedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [word_tokenize(sentence) for sentence in clean_text]\n",
    "word_to_ind = {}\n",
    "for i in range(len(tokenized_corpus)):\n",
    "    token_arr = tokenized_corpus[i]\n",
    "    \n",
    "    #Vocabulary\n",
    "    for tokken in token_arr:\n",
    "        if tokken not in word_to_ind:\n",
    "            word_to_ind[tokken] = len(word_to_ind)\n",
    "    \n",
    "    token_arr = ['<sos>'] * 5 + token_arr + ['<eos>'] * 5\n",
    "    tokenized_corpus[i] = token_arr\n",
    "\n",
    "# print(tokenized_corpus[2])\n",
    "word_to_ind[\"<sos>\"] = len(word_to_ind)\n",
    "word_to_ind[\"<eos>\"] = len(word_to_ind)\n",
    "print(len(word_to_ind))\n",
    "\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Test-Train Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_data, test_data = train_test_split(tokenized_corpus, test_size=0.2)\n",
    "\n",
    "train_data, validation_data = train_test_split(train_val_data, test_size=0.125)\n",
    "\n",
    "# Print the sizes of each set\n",
    "print(f\"Training data size: {len(train_data)}\")\n",
    "print(f\"Validation data size: {len(validation_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Neural Network Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_size, context_size, vocab_size, pretrained_embeddings):\n",
    "        super(NeuralLM, self).__init__()\n",
    "        self.embeddings = nn.Embedding.from_pretrained(torch.tensor(pretrained_embeddings), freeze=True)\n",
    "\n",
    "        self.l1 = torch.nn.Linear(context_size * emb_dim, hidden_size)\n",
    "        self.a1 = torch.nn.Tanh()\n",
    "        self.l2 = torch.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # Lookup embeddings using word indices\n",
    "        inp = self.embeddings(inp)\n",
    "        \n",
    "        inp = inp.view(inp.size(1), -1) \n",
    "        # print(inp.shape)\n",
    "        \n",
    "        inp = self.l1(inp)\n",
    "        inp = self.a1(inp)\n",
    "        inp = self.l2(inp)\n",
    "        return inp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, context_indices, next_word_indices):\n",
    "        self.context_indices = context_indices\n",
    "        self.next_word_indices = next_word_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.next_word_indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.context_indices[index]), torch.tensor(self.next_word_indices[index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating Input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_Gram = 5\n",
    "\n",
    "def process_sentences(sentences, word_to_index, context_size):\n",
    "    def words_to_indices(words, word_to_index):\n",
    "        return [word_to_index.get(word, 0) for word in words]  # Default to 0 if word not in vocab\n",
    "\n",
    "    context_indices = []\n",
    "    central_word_indices = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        word_indices = words_to_indices(sentence, word_to_index)\n",
    "\n",
    "        for i in range(len(sentence) - context_size):\n",
    "            context_window = word_indices[i:i + context_size]\n",
    "            context_indices.append(context_window)\n",
    "            central_word_indices.append(word_to_index.get(sentence[i + context_size], 0))\n",
    "\n",
    "    return context_indices, central_word_indices\n",
    "\n",
    "\n",
    "train_gram_inp, train_cen_inp = process_sentences(train_data, word_to_ind, N_Gram)\n",
    "val_gram_inp, val_cen_inp = process_sentences(validation_data, word_to_ind, N_Gram)\n",
    "test_gram_inp, test_cen_inp = process_sentences(test_data, word_to_ind, N_Gram)\n",
    "\n",
    "print(len(train_cen_inp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = EntityDataset(train_gram_inp, train_cen_inp)\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=128)\n",
    "\n",
    "dataset_val = EntityDataset(val_gram_inp, val_cen_inp)\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=128)\n",
    "\n",
    "pretrained_embeddings = word2vec_model.wv.vectors\n",
    "\n",
    "model = NeuralLM(100, 300, N_Gram, len(word_to_ind), pretrained_embeddings)\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader_train:\n",
    "        context_words, target_words = batch\n",
    "        trans_context = torch.transpose(context_words,0,1)\n",
    "        trans_context = trans_context.to(device)\n",
    "        target_words = target_words.to(device)\n",
    "\n",
    "        outputs = model(trans_context) \n",
    "        loss = criterion(outputs, target_words)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(dataloader_train)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader_val:\n",
    "            context_words, target_words = batch\n",
    "            trans_context = torch.transpose(context_words,0,1)\n",
    "            trans_context = trans_context.to(device)\n",
    "            target_words = target_words.to(device)\n",
    "            \n",
    "            outputs = model(trans_context)\n",
    "            loss = criterion(outputs, target_words)\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += target_words.size(0)\n",
    "            correct += (predicted == target_words).sum().item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(dataloader_val)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluate Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = EntityDataset(test_gram_inp, test_cen_inp)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=128)\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "total_loss = 0\n",
    "total_tokens = 0\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader_test:\n",
    "        context_words, target_words = batch\n",
    "        trans_context = torch.transpose(context_words,0,1)\n",
    "        trans_context = trans_context.to(device)\n",
    "        target_words = target_words.to(device)\n",
    "\n",
    "        outputs = model(trans_context)\n",
    "        # print(outputs)\n",
    "        \n",
    "        loss = criteria(outputs, target_words)\n",
    "        total_loss += loss.item()\n",
    "        # total_tokens += target_words.numel()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += target_words.size(0)\n",
    "        correct += (predicted == target_words).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "print(math.exp(total_loss/len(dataloader_test)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
