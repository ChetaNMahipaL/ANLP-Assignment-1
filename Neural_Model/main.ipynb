{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing and Cleaning Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35103\n"
     ]
    }
   ],
   "source": [
    "with open('./datasets/Auguste_Maquet.txt', 'r', encoding='utf-8') as file:\n",
    "    corpus = file.read()\n",
    "\n",
    "corpus = corpus.lower()\n",
    "clean_text = sent_tokenize(corpus)\n",
    "print(len(clean_text))\n",
    "filtered_corpus = [line for line in clean_text if line.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenization and Emmbedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29521\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus = [word_tokenize(sentence) for sentence in filtered_corpus]\n",
    "uniq_words = {}\n",
    "for i in range(len(tokenized_corpus)):\n",
    "    token_arr = tokenized_corpus[i]\n",
    "    \n",
    "    #Vocabulary\n",
    "    for tokken in token_arr:\n",
    "        if tokken not in uniq_words:\n",
    "            uniq_words[tokken] = len(uniq_words)\n",
    "    \n",
    "    token_arr = ['<sos>'] * 5 + token_arr + ['<eos>'] * 5\n",
    "    tokenized_corpus[i] = token_arr\n",
    "\n",
    "# print(tokenized_corpus[2])\n",
    "uniq_words[\"<sos>\"] = len(uniq_words)\n",
    "uniq_words[\"<eos>\"] = len(uniq_words)\n",
    "print(len(uniq_words))\n",
    "\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=tokenized_corpus, vector_size=300, window=5, min_count=1, workers=4)\n",
    "\n",
    "# sos_vector = word2vec_model.wv['<sos>']\n",
    "# print(sos_vector)\n",
    "\n",
    "# similarity = word2vec_model.wv.similarity('revolution', 'freedom')\n",
    "# print(similarity)\n",
    "\n",
    "# Get the entire embedding matrix\n",
    "# embedding_matrix = word2vec_model.wv.vectors\n",
    "# print(\"Embedding matrix shape:\", embedding_matrix.shape)\n",
    "\n",
    "# Find the word in the vocabulary that is closest to this vector\n",
    "# most_similar_word = word2vec_model.wv.similar_by_vector(embedding_matrix[263], topn=1)\n",
    "# print(most_similar_word[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Test-Train Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 24573\n",
      "Validation data size: 3510\n",
      "Test data size: 7020\n"
     ]
    }
   ],
   "source": [
    "train_val_data, test_data = train_test_split(tokenized_corpus, test_size=int(0.2*(len(tokenized_corpus))), random_state=42)\n",
    "\n",
    "# Then, split the remaining data into training and validation sets\n",
    "train_data, validation_data = train_test_split(train_val_data, test_size=int(0.1*(len(tokenized_corpus))), random_state=42)\n",
    "\n",
    "# Print the sizes of each set\n",
    "print(f\"Training data size: {len(train_data)}\")\n",
    "print(f\"Validation data size: {len(validation_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Neural Network Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self,emb_dim,hidden_size, context_size, vocab_size):\n",
    "        super(NeuralLM,self).__init__()\n",
    "        self.l1 = torch.nn.Linear(context_size*emb_dim, hidden_size)\n",
    "        self.a1 = torch.nn.Tanh()\n",
    "        self.l2 = torch.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inp = inp.to(device)  # Ensure input is on the same device as the model\n",
    "        inp = self.l1(inp)\n",
    "        inp = self.a1(inp)\n",
    "        inp = self.l2(inp)\n",
    "        return inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, concatenatedEmbeddings, nextWordIndices):\n",
    "        self.concatenatedEmbeddings = concatenatedEmbeddings\n",
    "        self.nextWordIndices = nextWordIndices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.nextWordIndices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.concatenatedEmbeddings[index]), torch.tensor(self.nextWordIndices[index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating Input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "946367\n"
     ]
    }
   ],
   "source": [
    "N_Gram = 5\n",
    "\n",
    "def process_sentences(sentences, w2v_model, context_size):\n",
    "\n",
    "    embedding_matrix = w2v_model.wv.vectors\n",
    "    word_to_index = {word: idx for idx, word in enumerate(w2v_model.wv.index_to_key)}\n",
    "\n",
    "    def words_to_indices(words, word_to_index):\n",
    "        return [word_to_index.get(word, 0) for word in words]  # Default to 0 if word not in vocab\n",
    "\n",
    "    concatenated_contexts = []\n",
    "    central_word = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        word_indices = words_to_indices(sentence, word_to_index)\n",
    "\n",
    "        embeddings = embedding_matrix[word_indices]\n",
    "\n",
    "        for i in range(len(sentence) - context_size):\n",
    "            context_window = embeddings[i:i + context_size]\n",
    "            concatenated_context = context_window.flatten()  \n",
    "            concatenated_contexts.append(concatenated_context)\n",
    "            central_word.append(uniq_words[sentence[i + context_size]])\n",
    "\n",
    "    concatenated_contexts = np.array(concatenated_contexts)\n",
    "    return concatenated_contexts, central_word\n",
    "\n",
    "train_gram_inp, train_cen_inp = process_sentences(train_data,word2vec_model,N_Gram)\n",
    "print(len(train_cen_inp))\n",
    "\n",
    "val_gram_inp, val_cen_inp = process_sentences(validation_data,word2vec_model,N_Gram)\n",
    "test_gram_inp, test_cen_inp = process_sentences(test_data,word2vec_model,N_Gram)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 4.4543, Val Loss: 4.3518, Val Accuracy: 31.47%\n",
      "Epoch [2/5], Train Loss: 4.0475, Val Loss: 4.3840, Val Accuracy: 31.81%\n",
      "Epoch [3/5], Train Loss: 3.8410, Val Loss: 4.4382, Val Accuracy: 31.78%\n",
      "Epoch [4/5], Train Loss: 3.6880, Val Loss: 4.4946, Val Accuracy: 31.68%\n",
      "Epoch [5/5], Train Loss: 3.5700, Val Loss: 4.5451, Val Accuracy: 31.63%\n",
      "Training and validation complete.\n"
     ]
    }
   ],
   "source": [
    "dataset_train = EntityDataset(train_gram_inp, train_cen_inp)\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=128)\n",
    "\n",
    "\n",
    "dataset_val = EntityDataset(val_gram_inp, val_cen_inp)\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=128)\n",
    "\n",
    "# Assuming NeuralLM is your FFNNPredictor or any other model that you've defined\n",
    "model = NeuralLM(300, 300, 5, len(uniq_words))  # Ensure inputSize matches concatenated embedding size\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "model.to(device)  # Move the model to GPU\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    total_loss = 0\n",
    "    for batch in dataloader_train:\n",
    "        concatenated_embeds, target_words = batch\n",
    "\n",
    "        concatenated_embeds = concatenated_embeds.to(device)\n",
    "        target_words = target_words.to(device)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear the gradients\n",
    "        \n",
    "        outputs = model(concatenated_embeds)  # Forward pass\n",
    "        \n",
    "        loss = criterion(outputs, target_words)  # Compute loss\n",
    "        \n",
    "        loss.backward()  # Backward pass (compute gradients)\n",
    "        optimizer.step()  # Update model parameters\n",
    "        \n",
    "        total_loss += loss.item()  # Accumulate loss\n",
    "    \n",
    "    avg_train_loss = total_loss / len(dataloader_train)\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for batch in dataloader_val:\n",
    "            concatenated_embeds, target_words = batch\n",
    "            concatenated_embeds = concatenated_embeds.to(device)\n",
    "            target_words = target_words.to(device)\n",
    "            \n",
    "            outputs = model(concatenated_embeds)  # Forward pass\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, target_words)\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += target_words.size(0)\n",
    "            correct += (predicted == target_words).sum().item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(dataloader_val)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "print(\"Training and validation complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluate Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.78429788484136\n"
     ]
    }
   ],
   "source": [
    "dataset_test = EntityDataset(test_gram_inp, test_cen_inp)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=128)\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "    for batch in dataloader_test:\n",
    "        concatenated_embeds, target_words = batch\n",
    "        \n",
    "        # Move data to GPU\n",
    "        concatenated_embeds = concatenated_embeds.to(device)\n",
    "        target_words = target_words.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(concatenated_embeds)\n",
    "        \n",
    "        # Get the predicted class\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Calculate number of correct predictions\n",
    "        total += target_words.size(0)\n",
    "        correct += (predicted == target_words).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
